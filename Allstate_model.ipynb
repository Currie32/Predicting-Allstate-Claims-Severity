{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for my best submission in Kaggle's \"Allstate Claims Severity\" competition. I found this competition rather interesting because I have never worked with such anonymized data before. The goal for this competition was to minimize the mean average error when predicting the values of the insurance claims (this feature is referred to as loss). If you would like to learn more about the competition, visit https://www.kaggle.com/c/allstate-claims-severity\n",
    "\n",
    "The sections of this analysis are:\n",
    "-Inspecting the data \n",
    "-Building the model\n",
    "-Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#display max rows and columns\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/Dave/Desktop/Programming/Personal Projects/Allstate-Kaggle/train.csv\")\n",
    "test = pd.read_csv(\"/Users/Dave/Desktop/Programming/Personal Projects/Allstate-Kaggle/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.describe(include = ['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train.shape\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good to see that there are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at our target feature, loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(train.loss, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a better picture of things by transforming loss by log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(train.loss, bins = 50)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "train.loss.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is quite the long-tail distribution here, let's see if a boxplot makes things a little easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot(train.loss, 1)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's surprising to see a loss claim for as low as 67 cents, but the majority of the data is between 1000 and 4000 dollars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the train and test datasets to inspect and transform the data quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([train,test], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group all of the continuous and categorical features together to make exploring and transforming the data easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cont_features = []\n",
    "cat_features = []\n",
    "\n",
    "for i in train.columns:\n",
    "    if train[i].dtype == 'float':\n",
    "        cont_features.append(i)\n",
    "    elif train[i].dtype == 'object':\n",
    "        cat_features.append(i)\n",
    "        \n",
    "for c in range(len(cat_features)):\n",
    "    train[cat_features[c]] = train[cat_features[c]].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cont_features = []\n",
    "cat_features = []\n",
    "\n",
    "for i in test.columns:\n",
    "    if test[i].dtype == 'float':\n",
    "        cont_features.append(i)\n",
    "    elif test[i].dtype == 'object':\n",
    "        cat_features.append(i)\n",
    "        \n",
    "for c in range(len(cat_features)):\n",
    "    test[cat_features[c]] = test[cat_features[c]].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cat in train[cat_features]:\n",
    "    print cat\n",
    "    print\n",
    "    print np.corrcoef(x = train.loss, y = train[cat])\n",
    "    plt.hist(train[cat])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the categorical features have uneven distributions. I suspect these feature are asking about what type of claim the customer has, i.e. auto, theft, fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,25))\n",
    "gs = gridspec.GridSpec(8, 2)\n",
    "for i, cn in enumerate(train[cont_features]):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    sns.distplot(train[cn], bins=50)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title('histogram of feature: ' + str(cn))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cont_corr = train[cont_features].corr()\n",
    "plt.subplots(figsize=(15, 12))\n",
    "sns.set(style=\"white\")\n",
    "mask = np.zeros_like(cont_corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(cont_corr, vmax=1, annot=True, mask = mask,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train[cont_features].corrwith(train.loss).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the continuous features have high correlations with each other, but nothing is very correlated with loss. I'm tempted to drop cont12 now because it is so highly correlated with cont11, but first I'm am going to look at how tranforming the features impact their correlation with loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in train[cont_features]:\n",
    "    print feature\n",
    "    print \"no transformation:\", train[feature].corr(train.loss)\n",
    "    print \"sqrt transformation:\", np.sqrt(train[feature]).corr(train.loss)\n",
    "    print \"log10 transformation:\", np.log10(train[feature]).corr(train.loss)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.cont1 = np.log10(train.cont1)\n",
    "train.cont2 = np.log10(train.cont2)\n",
    "train.cont4 = np.log10(train.cont4)\n",
    "train.cont5 = np.log10(train.cont5)\n",
    "train.cont8 = np.log10(train.cont8)\n",
    "train.cont10 = np.sqrt(train.cont10)\n",
    "train.cont13 = np.log10(train.cont13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.cont1 = np.log10(test.cont1)\n",
    "test.cont2 = np.log10(test.cont2)\n",
    "test.cont4 = np.log10(test.cont4)\n",
    "test.cont5 = np.log10(test.cont5)\n",
    "test.cont8 = np.log10(test.cont8)\n",
    "test.cont10 = np.sqrt(test.cont10)\n",
    "test.cont13 = np.log10(test.cont13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I did not transform either cont11 or cont12, I will drop cont12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.drop('cont12', 1)\n",
    "test = test.drop('cont12', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = train.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainFinal = train.drop('loss', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if train and test still have the same number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(trainFinal.columns)\n",
    "print len(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All's good there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Because we are transforming y (loss) by log, we want to square its value and \n",
    "#subtract the shift that we added to it initially.\n",
    "def log_xg_eval_mae(yhat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-shift,\n",
    "                                      np.exp(yhat)-shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_state = 2\n",
    "params = {\n",
    "        'eta': 0.03,\n",
    "        'gamma': 0.5,\n",
    "        'max_depth': 10,\n",
    "        'min_child_weight': 6,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'alpha': 1,\n",
    "        'lambda': 1.5,\n",
    "        'verbose_eval': True,\n",
    "        'seed': random_state,\n",
    "    }\n",
    "''' \n",
    "BEST PARAMETERS\n",
    "params = {\n",
    "        'eta': 0.03,\n",
    "        'gamma': 0.5,\n",
    "        'max_depth': 10,\n",
    "        'min_child_weight': 6,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'alpha': 1,\n",
    "        'lambda': 1.5,\n",
    "        'verbose_eval': True,\n",
    "        'seed': random_state,\n",
    "    }\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "n_folds = 10\n",
    "cv_sum = 0 #The sum of the mean_absolute_error for each fold.\n",
    "early_stopping_rounds = 100\n",
    "iterations = 10000\n",
    "shift = 250\n",
    "printN = 100\n",
    "fpred = [] #stores the sums of predicted values for each fold.\n",
    "\n",
    "trainScaled = trainFinal.apply(lambda x: MinMaxScaler().fit_transform(x))\n",
    "testScaled = test.apply(lambda x: MinMaxScaler().fit_transform(x))\n",
    "\n",
    "#Based on f-score and pearson-r, these feature are not useful in our prediction. \n",
    "#The code is written at the bottom of this analysis for these measures.\n",
    "trainScaled = trainScaled.drop(['id','cat15','cat22','cat64','cat70','cat86','cat93','cat97','cat107','cat108'], 1)\n",
    "testScaled = testScaled.drop(['id','cat15','cat22','cat64','cat70','cat86','cat93','cat97','cat107','cat108'], 1)\n",
    "\n",
    "testFinal = xgb.DMatrix(testScaled)\n",
    "ytrain = np.log(labels + shift) \n",
    "\n",
    "kf = KFold(trainScaled.shape[0], n_folds=n_folds)\n",
    "for i, (train_index, test_index) in enumerate(kf):\n",
    "    print('\\n Fold %d' % (i+1))\n",
    "    X_train, X_val = trainScaled.iloc[train_index], trainScaled.iloc[test_index]\n",
    "    y_train, y_val = ytrain.iloc[train_index], ytrain.iloc[test_index]\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    xgtest = xgb.DMatrix(X_val, label = y_val)\n",
    "    watchlist = [(xgtrain, 'train'), (xgtest, 'eval')] \n",
    "    \n",
    "    xgbModel = xgb.train(params, \n",
    "                         xgtrain, \n",
    "                         iterations, \n",
    "                         watchlist,\n",
    "                         verbose_eval = printN,\n",
    "                         early_stopping_rounds=early_stopping_rounds,\n",
    "                         feval = log_xg_eval_mae\n",
    "                        )\n",
    "    \n",
    "    scores_val = xgbModel.predict(xgtest, ntree_limit=xgbModel.best_ntree_limit)\n",
    "    cv_score = mean_absolute_error(np.exp(y_val), np.exp(scores_val))\n",
    "    print('log_eval-MAE: %.6f' % cv_score)\n",
    "    y_pred = np.exp(xgbModel.predict(testFinal, ntree_limit=xgbModel.best_ntree_limit)) - shift\n",
    "    print(xgbModel.best_ntree_limit)\n",
    "\n",
    "    if i > 0:\n",
    "        fpred = pred + y_pred\n",
    "    else:\n",
    "        fpred = y_pred\n",
    "    pred = fpred\n",
    "    cv_sum = cv_sum + cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mpred = pred / n_folds\n",
    "score = cv_sum / n_folds\n",
    "print('Average eval-MAE: %.6f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Writing results\")\n",
    "result = pd.DataFrame(mpred)\n",
    "result[id] = test.id\n",
    "result.columns = ['loss','id']\n",
    "result.to_csv('/Users/Dave/Desktop/Programming/Personal Projects/Allstate-Kaggle/result.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with anonymized presents different challenges than working with standard data. Without the ability to confidently engineer new features, I was limited in the ways I could optimize my transformations of the data and build my model. Nonetheless, I still believe that I did a good job in creating a useful model. My score for the Kaggle competition was 1130.01427 (mean average error), the winning score was 1109.70772, and the 'Random Forest Benchmark' score was 1227.74974. Clearly my score is far closer to that of the winner, but there was still room for improvement. If I created an ensemble of models, transformed my features differently, or even used different features in my model, my score could have improved.\n",
    "\n",
    "The five most important features in my model were: cont14, cont7, cont6, cat100, and cat112. Since I do not know what these features represent, we cannot draw much of a conclusion, but it's still a good practice to identify which features have the greatest impact on a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find the importance of each feature\n",
    "importance = xgbModel.get_fscore()\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print importance\n",
    "print len(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for feature in trainFinal:\n",
    "    print feature\n",
    "    print stats.pearsonr(trainFinal[feature], labels)\n",
    "    print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
